---
title: "Homework 7"
author: "Heywood Lau"
date: "2/22/2022"
output: html_document
---

```{r}
rm(list = ls())
my_data <- read.delim("C:/Users/heywo/OneDrive - Queen's University/Georgia Tech/ISYE 6501/Week 7 Homework/uscrime.txt")


#load packages
library(rpart)
library(rpart.plot)
```

```{r}

#decision tree model
reg_tree <- rpart(Crime ~., my_data)
summary(reg_tree)

#view the nature of the variables
str(reg_tree)

#view the  decision tree 
rpart.plot(reg_tree)

#based on the initial overview of the plot, per capita expenditure on police protection in 1960 is important, when the expenditure is less 7.7 per capita, the population is likely to be less than 2.3. For per capital expenditure>7.7, the it seems like the the non-white population is higher 

decision_tree_pred <- round(predict(reg_tree, my_data))
decision_tree_table<-table(my_data$Crime, decision_tree_pred)
dim(decision_tree_table)

print((decision_tree_table))

sum(diag(decision_tree_table))/sum(decision_tree_table)
#not a very successful decision tree model for evaluating a numeric example
```

```{r}
#random forest 
library(randomForest)

library(caret)

#partition data
set.seed(222)

#split the data into training
index <- sample(2, nrow(my_data), replace = TRUE, prob = c(0.7, 0.3))

#training data equals the 70% portion
train <- my_data[index==1,]

#testing is 30% portion
test <- my_data[index==2,]

#define the random forest model
random_forest<- randomForest(Crime~., data=train)

print(random_forest)
#only 43% of the variance could be explained using the random forest regression
plot(random_forest)

summary(random_forest)

prediction <- round(predict(random_forest, test, type = "response"))
summary(prediction)


table(round(prediction),test$Crime)   

sum(prediction ==test$Crime)/nrow(test)
#no visibility on predicitng crime rate
print(prediction)
```
```{r}
#Question 10.2 
#to use logistic regression to predict whether the S&P 500 is going to increase tomorrow. we could use predictors, such as number of IPOs, sentiment index(volatility), treasury bill interest rate(daily)
```


```{r}

library("ISLR")

credit_data <- read.delim("C:/Users/heywo/OneDrive - Queen's University/Georgia Tech/ISYE 6501/Week 7 Homework/germancredit.txt", header =FALSE, sep=" ")
head(credit_data)
summary(credit_data)

str(credit_data)

#0 is bad credit
for (i in 1:length(credit_data[,21])){
   if (credit_data[i,21] == 2)
    credit_data[i,21]<- 0
}

#fit the model with the logistic regression
glm.fit <- glm(V21~., data = credit_data, family = binomial)


summary(glm.fit)
#Null deviance is the deviance from the mean, and residual deviance is the deviance of the model with all the predictors 




#use the model train to predict based on probability 
glm_probability <- predict(glm.fit, type="response")
print(glm_probability)

#creating threshold to determine the good credit and bad credit
glm.predidction <- ifelse(glm_probability > 0.5, "Good_Credit", "Bad_Credit")

#access all the columns as standalone variables 
attach(credit_data)


table(glm.predidction, V21)
#as we can tell the our model shows pretty high accuracy 

#accuracy
(160+626)/(160+74+140+626)


```

